<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Liveness Detection</title>
    <style>
        :root {
            --primary-color: #4a90d9;
            --success-color: #4caf50;
            --error-color: #f44336;
            --warning-color: #ff9800;
            --bg-color: #1a1a2e;
            --card-bg: #16213e;
            --text-color: #ffffff;
            --text-muted: #a0a0a0;
            --border-radius: 12px;
            --transition-speed: 0.3s;
        }

        * {
            margin: 0;
            padding: 0;
            box-sizing: border-box;
        }

        body {
            font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, sans-serif;
            background: var(--bg-color);
            color: var(--text-color);
            min-height: 100vh;
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            padding: 20px;
        }

        .container {
            width: 100%;
            max-width: 600px;
            display: flex;
            flex-direction: column;
            align-items: center;
            gap: 20px;
        }

        /* Header Section */
        .header {
            text-align: center;
            width: 100%;
        }

        .header h1 {
            font-size: 1.8rem;
            font-weight: 600;
            margin-bottom: 8px;
        }

        .header p {
            color: var(--text-muted);
            font-size: 0.95rem;
        }

        /* Video Container */
        .video-container {
            position: relative;
            width: 100%;
            max-width: 480px;
            aspect-ratio: 4/3;
            background: var(--card-bg);
            border-radius: var(--border-radius);
            overflow: hidden;
            box-shadow: 0 8px 32px rgba(0, 0, 0, 0.3);
        }

        #video {
            width: 100%;
            height: 100%;
            object-fit: cover;
            transform: scaleX(-1); /* Mirror the video */
        }

        #canvas {
            position: absolute;
            top: 0;
            left: 0;
            width: 100%;
            height: 100%;
            pointer-events: none;
        }

        /* Face Guide Overlay */
        .face-guide {
            position: absolute;
            top: 50%;
            left: 50%;
            transform: translate(-50%, -50%);
            width: 55%;
            height: 70%;
            border: 3px dashed var(--primary-color);
            border-radius: 50%;
            opacity: 0.7;
            transition: border-color var(--transition-speed), opacity var(--transition-speed);
        }

        .face-guide.detected {
            border-color: var(--success-color);
            border-style: solid;
            opacity: 1;
        }

        .face-guide.warning {
            border-color: var(--warning-color);
        }

        /* Status Overlay */
        .status-overlay {
            position: absolute;
            bottom: 0;
            left: 0;
            right: 0;
            background: linear-gradient(transparent, rgba(0, 0, 0, 0.8));
            padding: 40px 20px 20px;
            text-align: center;
        }

        .status-text {
            font-size: 1.1rem;
            font-weight: 500;
        }

        /* Challenge Panel */
        .challenge-panel {
            width: 100%;
            max-width: 480px;
            background: var(--card-bg);
            border-radius: var(--border-radius);
            padding: 24px;
            box-shadow: 0 4px 16px rgba(0, 0, 0, 0.2);
        }

        .challenge-header {
            display: flex;
            align-items: center;
            justify-content: space-between;
            margin-bottom: 16px;
        }

        .challenge-title {
            font-size: 1.1rem;
            font-weight: 600;
        }

        .challenge-timer {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        .challenge-instruction {
            display: flex;
            align-items: center;
            gap: 16px;
            padding: 16px;
            background: rgba(74, 144, 217, 0.1);
            border-radius: 8px;
            margin-bottom: 16px;
        }

        .challenge-icon {
            font-size: 2.5rem;
            flex-shrink: 0;
        }

        .challenge-text {
            flex: 1;
        }

        .challenge-text h3 {
            font-size: 1.2rem;
            margin-bottom: 4px;
        }

        .challenge-text p {
            font-size: 0.9rem;
            color: var(--text-muted);
        }

        /* Progress Bar */
        .progress-container {
            width: 100%;
        }

        .progress-label {
            display: flex;
            justify-content: space-between;
            margin-bottom: 8px;
            font-size: 0.85rem;
            color: var(--text-muted);
        }

        .progress-bar {
            width: 100%;
            height: 8px;
            background: rgba(255, 255, 255, 0.1);
            border-radius: 4px;
            overflow: hidden;
        }

        .progress-fill {
            height: 100%;
            background: var(--primary-color);
            border-radius: 4px;
            width: 0%;
            transition: width var(--transition-speed);
        }

        .progress-fill.success {
            background: var(--success-color);
        }

        /* Progress Indicators */
        .progress-indicators {
            display: flex;
            justify-content: center;
            gap: 12px;
            margin-top: 16px;
        }

        .indicator {
            width: 12px;
            height: 12px;
            border-radius: 50%;
            background: rgba(255, 255, 255, 0.2);
            transition: background var(--transition-speed), transform var(--transition-speed);
        }

        .indicator.active {
            background: var(--primary-color);
            transform: scale(1.2);
        }

        .indicator.completed {
            background: var(--success-color);
        }

        .indicator.failed {
            background: var(--error-color);
        }

        /* Control Button */
        .control-btn {
            width: 100%;
            max-width: 480px;
            padding: 16px 32px;
            font-size: 1.1rem;
            font-weight: 600;
            color: white;
            background: var(--primary-color);
            border: none;
            border-radius: var(--border-radius);
            cursor: pointer;
            transition: background var(--transition-speed), transform 0.1s;
        }

        .control-btn:hover {
            background: #3a7bc8;
        }

        .control-btn:active {
            transform: scale(0.98);
        }

        .control-btn:disabled {
            background: var(--text-muted);
            cursor: not-allowed;
        }

        /* Result Screen */
        .result-screen {
            display: none;
            text-align: center;
            padding: 40px 20px;
        }

        .result-screen.visible {
            display: block;
        }

        .result-icon {
            font-size: 4rem;
            margin-bottom: 16px;
        }

        .result-title {
            font-size: 1.5rem;
            font-weight: 600;
            margin-bottom: 8px;
        }

        .result-message {
            color: var(--text-muted);
            margin-bottom: 24px;
        }

        /* Error Message */
        .error-message {
            display: none;
            background: rgba(244, 67, 54, 0.1);
            border: 1px solid var(--error-color);
            border-radius: 8px;
            padding: 16px;
            text-align: center;
            color: var(--error-color);
        }

        .error-message.visible {
            display: block;
        }

        /* Responsive Adjustments */
        @media (max-width: 480px) {
            .header h1 {
                font-size: 1.5rem;
            }

            .challenge-panel {
                padding: 16px;
            }

            .challenge-instruction {
                flex-direction: column;
                text-align: center;
            }

            .challenge-icon {
                font-size: 2rem;
            }
        }

        /* Loading State */
        .loading {
            display: flex;
            flex-direction: column;
            align-items: center;
            justify-content: center;
            gap: 16px;
            padding: 40px;
        }

        .spinner {
            width: 48px;
            height: 48px;
            border: 4px solid rgba(255, 255, 255, 0.1);
            border-top-color: var(--primary-color);
            border-radius: 50%;
            animation: spin 1s linear infinite;
        }

        @keyframes spin {
            to {
                transform: rotate(360deg);
            }
        }

        /* Hidden utility class */
        .hidden {
            display: none !important;
        }
    </style>
</head>
<body>
    <div class="container">
        <!-- Header -->
        <div class="header">
            <h1>Liveness Verification</h1>
            <p>Complete the challenges to verify you're a real person</p>
        </div>

        <!-- Video Container -->
        <div class="video-container">
            <video id="video" autoplay playsinline></video>
            <canvas id="canvas"></canvas>
            <div class="face-guide" id="faceGuide"></div>
            <div class="status-overlay">
                <p class="status-text" id="statusText">Initializing camera...</p>
            </div>
        </div>

        <!-- Error Message -->
        <div class="error-message" id="errorMessage">
            <p id="errorText">Camera access denied. Please allow camera access and refresh the page.</p>
        </div>

        <!-- Challenge Panel -->
        <div class="challenge-panel" id="challengePanel">
            <div class="challenge-header">
                <span class="challenge-title">Challenge <span id="challengeNumber">1</span> of 4</span>
                <span class="challenge-timer" id="challengeTimer">5s remaining</span>
            </div>
            <div class="challenge-instruction">
                <span class="challenge-icon" id="challengeIcon">üëÅ</span>
                <div class="challenge-text">
                    <h3 id="challengeName">Blink Your Eyes</h3>
                    <p id="challengeHint">Blink naturally 2-3 times</p>
                </div>
            </div>
            <div class="progress-container">
                <div class="progress-label">
                    <span>Hold progress</span>
                    <span id="holdPercent">0%</span>
                </div>
                <div class="progress-bar">
                    <div class="progress-fill" id="progressFill"></div>
                </div>
            </div>
            <div class="progress-indicators">
                <div class="indicator active" id="indicator1"></div>
                <div class="indicator" id="indicator2"></div>
                <div class="indicator" id="indicator3"></div>
                <div class="indicator" id="indicator4"></div>
            </div>
        </div>

        <!-- Control Button -->
        <button class="control-btn" id="controlBtn">Start Verification</button>

        <!-- Result Screen -->
        <div class="result-screen" id="resultScreen">
            <div class="result-icon" id="resultIcon">‚úì</div>
            <h2 class="result-title" id="resultTitle">Verification Complete</h2>
            <p class="result-message" id="resultMessage">You have been verified as a real person.</p>
            <button class="control-btn" id="retryBtn">Try Again</button>
        </div>
    </div>

    <!-- MediaPipe Tasks Vision CDN -->
    <script src="https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/vision_bundle.js" crossorigin="anonymous"></script>

    <script>
        // DOM Elements references
        const elements = {
            video: document.getElementById('video'),
            canvas: document.getElementById('canvas'),
            faceGuide: document.getElementById('faceGuide'),
            statusText: document.getElementById('statusText'),
            errorMessage: document.getElementById('errorMessage'),
            errorText: document.getElementById('errorText'),
            challengePanel: document.getElementById('challengePanel'),
            challengeNumber: document.getElementById('challengeNumber'),
            challengeTimer: document.getElementById('challengeTimer'),
            challengeIcon: document.getElementById('challengeIcon'),
            challengeName: document.getElementById('challengeName'),
            challengeHint: document.getElementById('challengeHint'),
            holdPercent: document.getElementById('holdPercent'),
            progressFill: document.getElementById('progressFill'),
            indicators: [
                document.getElementById('indicator1'),
                document.getElementById('indicator2'),
                document.getElementById('indicator3'),
                document.getElementById('indicator4')
            ],
            controlBtn: document.getElementById('controlBtn'),
            resultScreen: document.getElementById('resultScreen'),
            resultIcon: document.getElementById('resultIcon'),
            resultTitle: document.getElementById('resultTitle'),
            resultMessage: document.getElementById('resultMessage'),
            retryBtn: document.getElementById('retryBtn')
        };

        // FaceLandmarker configuration
        const FACE_LANDMARKER_MODEL_URL = 'https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task';

        // Global state
        let faceLandmarker = null;
        let isModelLoading = false;
        let isModelReady = false;
        let videoStream = null;
        let isCameraReady = false;

        // Detection loop state
        let isDetectionRunning = false;
        let detectionAnimationId = null;
        let lastDetectionTime = 0;
        let currentFaceResults = null;

        /**
         * Initialize FaceLandmarker with GPU delegate
         * Loads the model from CDN and configures it for face detection with blendshapes
         */
        async function initializeFaceLandmarker() {
            if (isModelLoading || isModelReady) return;

            isModelLoading = true;
            updateStatus('Loading face detection model...');

            try {
                const { FaceLandmarker, FilesetResolver } = vision;

                // Load the WASM files from CDN
                const filesetResolver = await FilesetResolver.forVisionTasks(
                    'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm'
                );

                // Create FaceLandmarker with GPU delegate and blendshapes enabled
                faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
                    baseOptions: {
                        modelAssetPath: FACE_LANDMARKER_MODEL_URL,
                        delegate: 'GPU' // Use GPU for better performance
                    },
                    runningMode: 'VIDEO',
                    numFaces: 1,
                    outputFaceBlendshapes: true, // Enable blendshapes for expression detection
                    outputFacialTransformationMatrixes: true // Enable for head pose estimation
                });

                isModelReady = true;
                isModelLoading = false;
                console.log('FaceLandmarker initialized successfully with GPU delegate');

                return true;
            } catch (error) {
                isModelLoading = false;
                console.error('Failed to initialize FaceLandmarker:', error);

                // Try fallback to CPU if GPU fails
                if (error.message && error.message.includes('GPU')) {
                    console.log('GPU delegate failed, attempting CPU fallback...');
                    return await initializeFaceLandmarkerCPU();
                }

                showError('Failed to load face detection model. Please refresh the page.');
                return false;
            }
        }

        /**
         * Fallback initialization with CPU delegate
         */
        async function initializeFaceLandmarkerCPU() {
            try {
                const { FaceLandmarker, FilesetResolver } = vision;

                const filesetResolver = await FilesetResolver.forVisionTasks(
                    'https://cdn.jsdelivr.net/npm/@mediapipe/tasks-vision@0.10.3/wasm'
                );

                faceLandmarker = await FaceLandmarker.createFromOptions(filesetResolver, {
                    baseOptions: {
                        modelAssetPath: FACE_LANDMARKER_MODEL_URL,
                        delegate: 'CPU'
                    },
                    runningMode: 'VIDEO',
                    numFaces: 1,
                    outputFaceBlendshapes: true,
                    outputFacialTransformationMatrixes: true
                });

                isModelReady = true;
                isModelLoading = false;
                console.log('FaceLandmarker initialized with CPU delegate (fallback)');

                return true;
            } catch (error) {
                console.error('CPU fallback also failed:', error);
                showError('Failed to load face detection model. Please try a different browser.');
                return false;
            }
        }

        /**
         * Update status text displayed to user
         */
        function updateStatus(message) {
            elements.statusText.textContent = message;
        }

        /**
         * Show error message to user
         */
        function showError(message) {
            elements.errorText.textContent = message;
            elements.errorMessage.classList.add('visible');
            elements.controlBtn.disabled = true;
        }

        /**
         * Hide error message
         */
        function hideError() {
            elements.errorMessage.classList.remove('visible');
        }

        /**
         * Check if FaceLandmarker is ready for detection
         */
        function isFaceLandmarkerReady() {
            return isModelReady && faceLandmarker !== null;
        }

        /**
         * Initialize WebRTC camera with getUserMedia
         * Requests camera access and sets up the video stream with mirrored view
         */
        async function initializeCamera() {
            updateStatus('Requesting camera access...');

            // Define video constraints for optimal face detection
            const constraints = {
                video: {
                    width: { ideal: 640, max: 1280 },
                    height: { ideal: 480, max: 720 },
                    facingMode: 'user', // Front-facing camera
                    frameRate: { ideal: 30, max: 60 }
                },
                audio: false
            };

            try {
                // Request camera access
                videoStream = await navigator.mediaDevices.getUserMedia(constraints);

                // Attach stream to video element
                elements.video.srcObject = videoStream;

                // Wait for video to be ready
                await new Promise((resolve, reject) => {
                    elements.video.onloadedmetadata = () => {
                        // Set canvas dimensions to match video
                        elements.canvas.width = elements.video.videoWidth;
                        elements.canvas.height = elements.video.videoHeight;
                        resolve();
                    };
                    elements.video.onerror = () => {
                        reject(new Error('Video failed to load'));
                    };
                });

                // Play the video
                await elements.video.play();

                isCameraReady = true;
                updateStatus('Camera ready');
                console.log(`Camera initialized: ${elements.video.videoWidth}x${elements.video.videoHeight}`);

                return true;
            } catch (error) {
                console.error('Camera initialization failed:', error);
                isCameraReady = false;

                // Handle specific error types
                if (error.name === 'NotAllowedError' || error.name === 'PermissionDeniedError') {
                    showError('Camera access denied. Please allow camera access and refresh the page.');
                } else if (error.name === 'NotFoundError' || error.name === 'DevicesNotFoundError') {
                    showError('No camera found. Please connect a camera and refresh the page.');
                } else if (error.name === 'NotReadableError' || error.name === 'TrackStartError') {
                    showError('Camera is in use by another application. Please close other apps using the camera.');
                } else if (error.name === 'OverconstrainedError') {
                    // Try again with less strict constraints
                    console.log('Constraints too strict, retrying with basic constraints...');
                    return await initializeCameraBasic();
                } else {
                    showError(`Camera error: ${error.message || 'Unknown error occurred'}`);
                }

                return false;
            }
        }

        /**
         * Fallback camera initialization with basic constraints
         * Used when ideal constraints cannot be satisfied
         */
        async function initializeCameraBasic() {
            const basicConstraints = {
                video: { facingMode: 'user' },
                audio: false
            };

            try {
                videoStream = await navigator.mediaDevices.getUserMedia(basicConstraints);
                elements.video.srcObject = videoStream;

                await new Promise((resolve, reject) => {
                    elements.video.onloadedmetadata = () => {
                        elements.canvas.width = elements.video.videoWidth;
                        elements.canvas.height = elements.video.videoHeight;
                        resolve();
                    };
                    elements.video.onerror = () => reject(new Error('Video failed to load'));
                });

                await elements.video.play();

                isCameraReady = true;
                updateStatus('Camera ready (basic mode)');
                console.log(`Camera initialized (basic): ${elements.video.videoWidth}x${elements.video.videoHeight}`);

                return true;
            } catch (error) {
                console.error('Basic camera initialization also failed:', error);
                showError('Unable to access camera. Please check your browser settings.');
                return false;
            }
        }

        /**
         * Stop the camera stream and release resources
         */
        function stopCamera() {
            if (videoStream) {
                videoStream.getTracks().forEach(track => {
                    track.stop();
                    console.log(`Stopped track: ${track.kind}`);
                });
                videoStream = null;
            }
            elements.video.srcObject = null;
            isCameraReady = false;
        }

        /**
         * Check if camera is ready for use
         */
        function isCameraInitialized() {
            return isCameraReady && videoStream !== null && elements.video.readyState >= 2;
        }

        /**
         * Check if both camera and model are ready
         */
        function isAppReady() {
            return isCameraInitialized() && isFaceLandmarkerReady();
        }

        /**
         * Enable the start button when both camera and model are ready
         */
        function checkAndEnableStartButton() {
            if (isAppReady()) {
                elements.controlBtn.disabled = false;
                updateStatus('Position your face in the oval and click Start');
            }
        }

        /**
         * Main detection loop using requestAnimationFrame
         * Continuously processes video frames for face detection
         */
        function detectionLoop(timestamp) {
            if (!isDetectionRunning) {
                return;
            }

            // Check if video is ready and playing
            if (!isCameraInitialized() || elements.video.paused || elements.video.ended) {
                detectionAnimationId = requestAnimationFrame(detectionLoop);
                return;
            }

            // Run face detection
            if (isFaceLandmarkerReady()) {
                try {
                    // Detect faces in the current video frame
                    // Use performance.now() for VIDEO running mode
                    const startTime = performance.now();
                    currentFaceResults = faceLandmarker.detectForVideo(elements.video, startTime);
                    lastDetectionTime = startTime;

                    // Process detection results
                    processDetectionResults(currentFaceResults);
                } catch (error) {
                    console.error('Detection error:', error);
                }
            }

            // Schedule next frame
            detectionAnimationId = requestAnimationFrame(detectionLoop);
        }

        /**
         * Process face detection results
         * Updates UI based on whether a face is detected and its position
         */
        function processDetectionResults(results) {
            const ctx = elements.canvas.getContext('2d');

            // Clear previous drawings
            ctx.clearRect(0, 0, elements.canvas.width, elements.canvas.height);

            // Process blink detection
            const blinkResult = BlinkDetector.processFrame(results);

            // Process head movement detection
            const headResult = HeadMovementDetector.processFrame(results);

            // Process expression detection
            const expressionResult = ExpressionDetector.processFrame(results);

            // Check if a face was detected
            if (results && results.faceLandmarks && results.faceLandmarks.length > 0) {
                const landmarks = results.faceLandmarks[0];
                const blendshapes = results.faceBlendshapes && results.faceBlendshapes.length > 0
                    ? results.faceBlendshapes[0].categories
                    : null;
                const transformMatrix = results.facialTransformationMatrixes && results.facialTransformationMatrixes.length > 0
                    ? results.facialTransformationMatrixes[0]
                    : null;

                // Update face guide to show detected state
                elements.faceGuide.classList.add('detected');
                elements.faceGuide.classList.remove('warning');

                // Check if face is well-positioned
                const facePosition = checkFacePosition(landmarks);
                if (!facePosition.isWellPositioned) {
                    elements.faceGuide.classList.add('warning');
                    elements.faceGuide.classList.remove('detected');
                    updateStatus(facePosition.message);
                } else {
                    // Show expression values for debugging/feedback
                    const smileDisplay = (expressionResult.values.smile * 100).toFixed(0);
                    const mouthDisplay = (expressionResult.values.mouthOpen * 100).toFixed(0);
                    const browDisplay = (expressionResult.values.eyebrowRaise * 100).toFixed(0);
                    const calibStatus = headResult.isCalibrated ? '' : ' (calibrating...)';
                    updateStatus(`Smile: ${smileDisplay}% | Mouth: ${mouthDisplay}% | Brow: ${browDisplay}%${calibStatus}`);
                }

                // Draw face landmarks on canvas (mirrored to match video)
                drawFaceLandmarks(ctx, landmarks);
            } else {
                // No face detected
                elements.faceGuide.classList.remove('detected');
                elements.faceGuide.classList.remove('warning');
                updateStatus('No face detected - Position your face in the oval');
                currentFaceResults = null;
            }
        }

        /**
         * Check if the face is well-positioned within the guide oval
         * Returns position status and guidance message
         */
        function checkFacePosition(landmarks) {
            // Get face bounding box from landmarks
            // Nose tip (landmark 1) as center reference
            const noseTip = landmarks[1];

            // Calculate face bounds
            let minX = 1, maxX = 0, minY = 1, maxY = 0;
            landmarks.forEach(point => {
                minX = Math.min(minX, point.x);
                maxX = Math.max(maxX, point.x);
                minY = Math.min(minY, point.y);
                maxY = Math.max(maxY, point.y);
            });

            const faceWidth = maxX - minX;
            const faceHeight = maxY - minY;
            const faceCenterX = (minX + maxX) / 2;
            const faceCenterY = (minY + maxY) / 2;

            // Define acceptable ranges (normalized 0-1)
            const centerTolerance = 0.15;
            const minFaceSize = 0.25;
            const maxFaceSize = 0.75;

            // Check horizontal centering (note: x is mirrored)
            if (Math.abs(faceCenterX - 0.5) > centerTolerance) {
                const direction = faceCenterX < 0.5 ? 'right' : 'left';
                return { isWellPositioned: false, message: `Move your face ${direction}` };
            }

            // Check vertical centering
            if (Math.abs(faceCenterY - 0.5) > centerTolerance) {
                const direction = faceCenterY < 0.5 ? 'down' : 'up';
                return { isWellPositioned: false, message: `Move your face ${direction}` };
            }

            // Check face size (too far or too close)
            if (faceWidth < minFaceSize || faceHeight < minFaceSize) {
                return { isWellPositioned: false, message: 'Move closer to the camera' };
            }

            if (faceWidth > maxFaceSize || faceHeight > maxFaceSize) {
                return { isWellPositioned: false, message: 'Move further from the camera' };
            }

            return { isWellPositioned: true, message: 'Face detected - Ready' };
        }

        /**
         * Draw face landmarks on the canvas
         * Draws contours for face mesh visualization
         */
        function drawFaceLandmarks(ctx, landmarks) {
            const width = elements.canvas.width;
            const height = elements.canvas.height;

            // Save context state
            ctx.save();

            // Mirror the canvas to match the mirrored video
            ctx.translate(width, 0);
            ctx.scale(-1, 1);

            // Set drawing style
            ctx.fillStyle = 'rgba(74, 144, 217, 0.6)';
            ctx.strokeStyle = 'rgba(74, 144, 217, 0.8)';
            ctx.lineWidth = 1;

            // Draw key facial contours
            const contours = {
                // Face oval
                faceOval: [10, 338, 297, 332, 284, 251, 389, 356, 454, 323, 361, 288, 397, 365, 379, 378, 400, 377, 152, 148, 176, 149, 150, 136, 172, 58, 132, 93, 234, 127, 162, 21, 54, 103, 67, 109, 10],
                // Left eye
                leftEye: [33, 7, 163, 144, 145, 153, 154, 155, 133, 173, 157, 158, 159, 160, 161, 246, 33],
                // Right eye
                rightEye: [362, 382, 381, 380, 374, 373, 390, 249, 263, 466, 388, 387, 386, 385, 384, 398, 362],
                // Lips outer
                lipsOuter: [61, 146, 91, 181, 84, 17, 314, 405, 321, 375, 291, 409, 270, 269, 267, 0, 37, 39, 40, 185, 61]
            };

            // Draw each contour
            Object.values(contours).forEach(indices => {
                ctx.beginPath();
                indices.forEach((index, i) => {
                    const point = landmarks[index];
                    const x = point.x * width;
                    const y = point.y * height;
                    if (i === 0) {
                        ctx.moveTo(x, y);
                    } else {
                        ctx.lineTo(x, y);
                    }
                });
                ctx.stroke();
            });

            // Draw landmark points for key features
            const keyPoints = [1, 33, 133, 362, 263, 61, 291]; // Nose, eyes, mouth corners
            keyPoints.forEach(index => {
                const point = landmarks[index];
                const x = point.x * width;
                const y = point.y * height;
                ctx.beginPath();
                ctx.arc(x, y, 3, 0, Math.PI * 2);
                ctx.fill();
            });

            // Restore context state
            ctx.restore();
        }

        /**
         * Start the detection loop
         */
        function startDetectionLoop() {
            if (isDetectionRunning) {
                console.log('Detection loop already running');
                return;
            }

            if (!isAppReady()) {
                console.error('Cannot start detection: app not ready');
                return;
            }

            isDetectionRunning = true;
            console.log('Starting detection loop');
            detectionAnimationId = requestAnimationFrame(detectionLoop);
        }

        /**
         * Stop the detection loop
         */
        function stopDetectionLoop() {
            isDetectionRunning = false;

            if (detectionAnimationId) {
                cancelAnimationFrame(detectionAnimationId);
                detectionAnimationId = null;
            }

            // Clear canvas
            const ctx = elements.canvas.getContext('2d');
            ctx.clearRect(0, 0, elements.canvas.width, elements.canvas.height);

            console.log('Detection loop stopped');
        }

        /**
         * Get the current face detection results
         * Used by other modules (BlinkDetector, HeadMovementDetector, etc.)
         */
        function getCurrentFaceResults() {
            return currentFaceResults;
        }

        /**
         * Check if detection is currently running
         */
        function isDetectionActive() {
            return isDetectionRunning;
        }

        // =====================================================
        // BLINK DETECTOR MODULE
        // Detects blinks using Eye Aspect Ratio (EAR) and blendshapes
        // =====================================================

        const BlinkDetector = (function() {
            // Configuration
            const config = {
                // EAR threshold - below this value indicates a blink
                earThreshold: 0.21,
                // Blendshape threshold - above this value indicates a blink
                blendshapeThreshold: 0.5,
                // Minimum consecutive frames to confirm a blink
                minConsecutiveFrames: 2,
                // Minimum time between blinks (ms) to avoid double-counting
                minTimeBetweenBlinks: 150,
                // Maximum blink duration (ms) - longer than this is considered eyes closed
                maxBlinkDuration: 500
            };

            // State
            let state = {
                isBlinking: false,
                blinkStartTime: null,
                consecutiveBlinkFrames: 0,
                lastBlinkTime: 0,
                blinkCount: 0,
                currentEAR: { left: 1, right: 1, average: 1 },
                currentBlendshapes: { left: 0, right: 0 }
            };

            // Eye landmark indices for EAR calculation
            // Based on MediaPipe Face Mesh 478 landmarks
            const eyeIndices = {
                // Left eye landmarks (from viewer's perspective, which is right eye anatomically)
                leftEye: {
                    // Vertical landmarks (top to bottom)
                    top: [159, 158, 157],      // Upper eyelid
                    bottom: [145, 144, 163],   // Lower eyelid
                    // Horizontal landmarks (inner to outer)
                    inner: 133,
                    outer: 33
                },
                // Right eye landmarks (from viewer's perspective, which is left eye anatomically)
                rightEye: {
                    top: [386, 385, 384],      // Upper eyelid
                    bottom: [374, 373, 380],   // Lower eyelid
                    inner: 362,
                    outer: 263
                }
            };

            /**
             * Calculate Eye Aspect Ratio (EAR) for one eye
             * EAR = (|p2-p6| + |p3-p5|) / (2 * |p1-p4|)
             * where p1-p6 are the eye landmark points
             */
            function calculateEAR(landmarks, eyeConfig) {
                // Get landmark points
                const top = eyeConfig.top.map(i => landmarks[i]);
                const bottom = eyeConfig.bottom.map(i => landmarks[i]);
                const inner = landmarks[eyeConfig.inner];
                const outer = landmarks[eyeConfig.outer];

                // Calculate vertical distances (average of 3 pairs)
                let verticalSum = 0;
                for (let i = 0; i < 3; i++) {
                    verticalSum += distance3D(top[i], bottom[i]);
                }
                const avgVertical = verticalSum / 3;

                // Calculate horizontal distance
                const horizontal = distance3D(inner, outer);

                // Calculate EAR
                if (horizontal === 0) return 1; // Avoid division by zero
                return avgVertical / horizontal;
            }

            /**
             * Calculate 3D Euclidean distance between two landmark points
             */
            function distance3D(p1, p2) {
                const dx = p1.x - p2.x;
                const dy = p1.y - p2.y;
                const dz = (p1.z || 0) - (p2.z || 0);
                return Math.sqrt(dx * dx + dy * dy + dz * dz);
            }

            /**
             * Get blink blendshape values from face detection results
             */
            function getBlinkBlendshapes(blendshapes) {
                if (!blendshapes || !Array.isArray(blendshapes)) {
                    return { left: 0, right: 0 };
                }

                let leftBlink = 0;
                let rightBlink = 0;

                // Find eyeBlink blendshapes in the categories array
                for (const shape of blendshapes) {
                    if (shape.categoryName === 'eyeBlinkLeft') {
                        leftBlink = shape.score;
                    } else if (shape.categoryName === 'eyeBlinkRight') {
                        rightBlink = shape.score;
                    }
                }

                return { left: leftBlink, right: rightBlink };
            }

            /**
             * Check if a blink is detected using both EAR and blendshapes
             * Returns true if either method detects a blink
             */
            function isBlinkDetected(earLeft, earRight, blendshapeLeft, blendshapeRight) {
                // Check EAR method
                const earAvg = (earLeft + earRight) / 2;
                const earBlink = earAvg < config.earThreshold;

                // Check blendshape method
                const blendshapeBlink = blendshapeLeft > config.blendshapeThreshold ||
                                       blendshapeRight > config.blendshapeThreshold;

                return earBlink || blendshapeBlink;
            }

            /**
             * Process a frame and detect blinks
             * Call this function every frame with the current face results
             * @param {Object} faceResults - Results from FaceLandmarker.detectForVideo()
             * @returns {Object} Detection result with blink status and metrics
             */
            function processFrame(faceResults) {
                const now = performance.now();

                // Default result
                const result = {
                    blinkDetected: false,
                    isCurrentlyBlinking: false,
                    blinkCount: state.blinkCount,
                    ear: state.currentEAR,
                    blendshapes: state.currentBlendshapes,
                    confidence: 0
                };

                // Check if we have valid face data
                if (!faceResults || !faceResults.faceLandmarks || faceResults.faceLandmarks.length === 0) {
                    // Reset state when no face detected
                    state.isBlinking = false;
                    state.consecutiveBlinkFrames = 0;
                    state.blinkStartTime = null;
                    return result;
                }

                const landmarks = faceResults.faceLandmarks[0];
                const blendshapes = faceResults.faceBlendshapes && faceResults.faceBlendshapes.length > 0
                    ? faceResults.faceBlendshapes[0].categories
                    : null;

                // Calculate EAR for both eyes
                const earLeft = calculateEAR(landmarks, eyeIndices.leftEye);
                const earRight = calculateEAR(landmarks, eyeIndices.rightEye);
                const earAvg = (earLeft + earRight) / 2;

                // Get blendshape values
                const blendshapeValues = getBlinkBlendshapes(blendshapes);

                // Update current values
                state.currentEAR = { left: earLeft, right: earRight, average: earAvg };
                state.currentBlendshapes = blendshapeValues;
                result.ear = state.currentEAR;
                result.blendshapes = state.currentBlendshapes;

                // Check if blink is detected this frame
                const blinkThisFrame = isBlinkDetected(
                    earLeft, earRight,
                    blendshapeValues.left, blendshapeValues.right
                );

                result.isCurrentlyBlinking = blinkThisFrame;

                if (blinkThisFrame) {
                    state.consecutiveBlinkFrames++;

                    // Start tracking blink time
                    if (!state.blinkStartTime) {
                        state.blinkStartTime = now;
                    }

                    // Calculate confidence based on how strongly the blink is detected
                    const earConfidence = Math.max(0, 1 - (earAvg / config.earThreshold));
                    const blendshapeConfidence = Math.max(blendshapeValues.left, blendshapeValues.right);
                    result.confidence = Math.max(earConfidence, blendshapeConfidence);

                    // Check if we've reached the minimum consecutive frames
                    if (state.consecutiveBlinkFrames >= config.minConsecutiveFrames && !state.isBlinking) {
                        // Check if enough time has passed since last blink
                        if (now - state.lastBlinkTime > config.minTimeBetweenBlinks) {
                            state.isBlinking = true;
                        }
                    }
                } else {
                    // Eyes are open now
                    if (state.isBlinking) {
                        // Blink just ended - check if it was a valid blink duration
                        const blinkDuration = now - (state.blinkStartTime || now);

                        if (blinkDuration < config.maxBlinkDuration) {
                            // Valid blink completed
                            state.blinkCount++;
                            state.lastBlinkTime = now;
                            result.blinkDetected = true;
                            result.blinkCount = state.blinkCount;
                            console.log(`Blink detected! Count: ${state.blinkCount}, Duration: ${blinkDuration.toFixed(0)}ms`);
                        }
                    }

                    // Reset blink tracking
                    state.isBlinking = false;
                    state.consecutiveBlinkFrames = 0;
                    state.blinkStartTime = null;
                }

                return result;
            }

            /**
             * Reset the blink detector state
             * Call this when starting a new challenge
             */
            function reset() {
                state = {
                    isBlinking: false,
                    blinkStartTime: null,
                    consecutiveBlinkFrames: 0,
                    lastBlinkTime: 0,
                    blinkCount: 0,
                    currentEAR: { left: 1, right: 1, average: 1 },
                    currentBlendshapes: { left: 0, right: 0 }
                };
                console.log('BlinkDetector reset');
            }

            /**
             * Get the current blink count
             */
            function getBlinkCount() {
                return state.blinkCount;
            }

            /**
             * Get the current EAR values
             */
            function getCurrentEAR() {
                return state.currentEAR;
            }

            /**
             * Get the current blendshape values
             */
            function getCurrentBlendshapes() {
                return state.currentBlendshapes;
            }

            /**
             * Check if currently in a blink
             */
            function isCurrentlyBlinking() {
                return state.isBlinking;
            }

            /**
             * Update configuration
             */
            function setConfig(newConfig) {
                Object.assign(config, newConfig);
            }

            /**
             * Get current configuration
             */
            function getConfig() {
                return { ...config };
            }

            // Public API
            return {
                processFrame,
                reset,
                getBlinkCount,
                getCurrentEAR,
                getCurrentBlendshapes,
                isCurrentlyBlinking,
                setConfig,
                getConfig
            };
        })();

        // =====================================================
        // HEAD MOVEMENT DETECTOR MODULE
        // Detects head yaw (left/right) and pitch (up/down) movements
        // Uses nose tip position relative to eye positions for calculation
        // =====================================================

        const HeadMovementDetector = (function() {
            // Configuration
            const config = {
                // Yaw threshold in degrees - head must turn this much from baseline
                yawThreshold: 15,
                // Pitch threshold in degrees - head must tilt this much from baseline
                pitchThreshold: 12,
                // Hold duration required in milliseconds
                holdDuration: 500,
                // Smoothing factor for angle calculations (0-1, higher = more smoothing)
                smoothingFactor: 0.3,
                // Number of frames to establish baseline
                calibrationFrames: 30,
                // Maximum allowed deviation during hold (degrees)
                holdTolerance: 5
            };

            // State
            let state = {
                // Baseline values (established during calibration)
                baseline: {
                    yaw: 0,
                    pitch: 0,
                    isCalibrated: false,
                    calibrationSamples: []
                },
                // Current smoothed values
                currentYaw: 0,
                currentPitch: 0,
                // Raw values (before smoothing)
                rawYaw: 0,
                rawPitch: 0,
                // Hold tracking
                holdStartTime: null,
                holdDirection: null, // 'left', 'right', 'up', 'down'
                holdProgress: 0,
                // Detection results
                lastDetectedMovement: null,
                movementHistory: []
            };

            // Key landmark indices for head pose estimation
            const landmarks = {
                noseTip: 1,
                noseBridge: 6,
                leftEyeInner: 133,
                leftEyeOuter: 33,
                rightEyeInner: 362,
                rightEyeOuter: 263,
                leftEyeCenter: 468,  // Iris center (if available)
                rightEyeCenter: 473, // Iris center (if available)
                foreheadCenter: 10,
                chin: 152
            };

            /**
             * Calculate yaw angle (left/right rotation) from landmarks
             * Uses the horizontal offset of nose tip relative to eye midpoint
             */
            function calculateYaw(faceLandmarks) {
                const noseTip = faceLandmarks[landmarks.noseTip];
                const leftEye = faceLandmarks[landmarks.leftEyeInner];
                const rightEye = faceLandmarks[landmarks.rightEyeInner];

                // Calculate eye midpoint
                const eyeMidpoint = {
                    x: (leftEye.x + rightEye.x) / 2,
                    y: (leftEye.y + rightEye.y) / 2,
                    z: ((leftEye.z || 0) + (rightEye.z || 0)) / 2
                };

                // Calculate eye width for normalization
                const eyeWidth = Math.abs(leftEye.x - rightEye.x);
                if (eyeWidth === 0) return 0;

                // Calculate horizontal offset of nose from eye midpoint
                const noseOffset = noseTip.x - eyeMidpoint.x;

                // Normalize by eye width and convert to approximate degrees
                // A rough approximation: full eye width offset ‚âà 45 degrees
                const normalizedOffset = noseOffset / eyeWidth;
                const yawDegrees = normalizedOffset * 45;

                // Also factor in z-depth difference if available
                const depthDiff = (noseTip.z || 0) - eyeMidpoint.z;
                const depthFactor = depthDiff * 30; // Add depth contribution

                return yawDegrees + depthFactor;
            }

            /**
             * Calculate pitch angle (up/down tilt) from landmarks
             * Uses vertical positioning of nose tip relative to eyes and forehead
             */
            function calculatePitch(faceLandmarks) {
                const noseTip = faceLandmarks[landmarks.noseTip];
                const noseBridge = faceLandmarks[landmarks.noseBridge];
                const forehead = faceLandmarks[landmarks.foreheadCenter];
                const chin = faceLandmarks[landmarks.chin];

                // Calculate face height for normalization
                const faceHeight = Math.abs(forehead.y - chin.y);
                if (faceHeight === 0) return 0;

                // Calculate the vertical ratio of nose tip position
                // between nose bridge and chin
                const noseVerticalRange = chin.y - noseBridge.y;
                if (noseVerticalRange === 0) return 0;

                const nosePosition = (noseTip.y - noseBridge.y) / noseVerticalRange;

                // Expected position is around 0.5 when looking straight
                // Higher value = looking down, Lower value = looking up
                const expectedPosition = 0.5;
                const deviation = nosePosition - expectedPosition;

                // Convert to approximate degrees (full range ‚âà 60 degrees)
                let pitchDegrees = deviation * 60;

                // Also use z-depth as additional signal
                const zDiff = (noseTip.z || 0) - (noseBridge.z || 0);
                // When looking down, nose tip z increases relative to bridge
                pitchDegrees += zDiff * 20;

                return pitchDegrees;
            }

            /**
             * Apply exponential smoothing to reduce noise
             */
            function smoothValue(currentSmoothed, newRaw, factor) {
                return currentSmoothed * (1 - factor) + newRaw * factor;
            }

            /**
             * Determine which direction the head is turned/tilted
             */
            function getMovementDirection(yaw, pitch, baseline) {
                const yawDelta = yaw - baseline.yaw;
                const pitchDelta = pitch - baseline.pitch;

                // Check yaw (left/right)
                if (Math.abs(yawDelta) >= config.yawThreshold) {
                    // Positive yaw = turned right (nose points to viewer's left due to mirroring)
                    return yawDelta > 0 ? 'right' : 'left';
                }

                // Check pitch (up/down)
                if (Math.abs(pitchDelta) >= config.pitchThreshold) {
                    // Positive pitch = looking down
                    return pitchDelta > 0 ? 'down' : 'up';
                }

                return null;
            }

            /**
             * Check if the current angle is within tolerance of the hold angle
             */
            function isHoldingPosition(currentDirection, yaw, pitch, baseline) {
                const yawDelta = yaw - baseline.yaw;
                const pitchDelta = pitch - baseline.pitch;

                switch (currentDirection) {
                    case 'left':
                        return yawDelta <= -config.yawThreshold + config.holdTolerance;
                    case 'right':
                        return yawDelta >= config.yawThreshold - config.holdTolerance;
                    case 'up':
                        return pitchDelta <= -config.pitchThreshold + config.holdTolerance;
                    case 'down':
                        return pitchDelta >= config.pitchThreshold - config.holdTolerance;
                    default:
                        return false;
                }
            }

            /**
             * Process a frame and detect head movements
             * @param {Object} faceResults - Results from FaceLandmarker.detectForVideo()
             * @returns {Object} Detection result with movement status and metrics
             */
            function processFrame(faceResults) {
                const now = performance.now();

                // Default result
                const result = {
                    isCalibrated: state.baseline.isCalibrated,
                    currentYaw: state.currentYaw,
                    currentPitch: state.currentPitch,
                    yawFromBaseline: 0,
                    pitchFromBaseline: 0,
                    direction: null,
                    isHolding: false,
                    holdProgress: 0,
                    movementDetected: false,
                    detectedDirection: null
                };

                // Check if we have valid face data
                if (!faceResults || !faceResults.faceLandmarks || faceResults.faceLandmarks.length === 0) {
                    // Reset hold state when no face detected
                    state.holdStartTime = null;
                    state.holdDirection = null;
                    state.holdProgress = 0;
                    return result;
                }

                const faceLandmarks = faceResults.faceLandmarks[0];

                // Calculate raw yaw and pitch
                state.rawYaw = calculateYaw(faceLandmarks);
                state.rawPitch = calculatePitch(faceLandmarks);

                // Apply smoothing
                state.currentYaw = smoothValue(state.currentYaw, state.rawYaw, config.smoothingFactor);
                state.currentPitch = smoothValue(state.currentPitch, state.rawPitch, config.smoothingFactor);

                result.currentYaw = state.currentYaw;
                result.currentPitch = state.currentPitch;

                // Handle calibration
                if (!state.baseline.isCalibrated) {
                    state.baseline.calibrationSamples.push({
                        yaw: state.currentYaw,
                        pitch: state.currentPitch
                    });

                    if (state.baseline.calibrationSamples.length >= config.calibrationFrames) {
                        // Calculate average baseline from samples
                        const samples = state.baseline.calibrationSamples;
                        state.baseline.yaw = samples.reduce((sum, s) => sum + s.yaw, 0) / samples.length;
                        state.baseline.pitch = samples.reduce((sum, s) => sum + s.pitch, 0) / samples.length;
                        state.baseline.isCalibrated = true;
                        console.log(`HeadMovementDetector calibrated - Baseline: yaw=${state.baseline.yaw.toFixed(1)}¬∞, pitch=${state.baseline.pitch.toFixed(1)}¬∞`);
                    }

                    result.isCalibrated = state.baseline.isCalibrated;
                    return result;
                }

                // Calculate deltas from baseline
                result.yawFromBaseline = state.currentYaw - state.baseline.yaw;
                result.pitchFromBaseline = state.currentPitch - state.baseline.pitch;
                result.isCalibrated = true;

                // Determine current movement direction
                const direction = getMovementDirection(state.currentYaw, state.currentPitch, state.baseline);
                result.direction = direction;

                // Handle hold detection
                if (direction) {
                    if (state.holdDirection === direction) {
                        // Continue holding same direction
                        if (isHoldingPosition(direction, state.currentYaw, state.currentPitch, state.baseline)) {
                            const holdTime = now - state.holdStartTime;
                            state.holdProgress = Math.min(holdTime / config.holdDuration, 1);
                            result.holdProgress = state.holdProgress;
                            result.isHolding = true;

                            // Check if hold is complete
                            if (holdTime >= config.holdDuration) {
                                result.movementDetected = true;
                                result.detectedDirection = direction;
                                state.lastDetectedMovement = direction;
                                state.movementHistory.push({
                                    direction: direction,
                                    timestamp: now
                                });
                                console.log(`Head movement detected: ${direction} (held for ${config.holdDuration}ms)`);

                                // Reset hold to allow detecting again
                                state.holdStartTime = null;
                                state.holdDirection = null;
                                state.holdProgress = 0;
                            }
                        } else {
                            // Position drifted out of tolerance - reset
                            state.holdStartTime = now;
                            state.holdProgress = 0;
                        }
                    } else {
                        // New direction - start new hold
                        state.holdDirection = direction;
                        state.holdStartTime = now;
                        state.holdProgress = 0;
                    }
                } else {
                    // No significant movement - reset hold state
                    state.holdStartTime = null;
                    state.holdDirection = null;
                    state.holdProgress = 0;
                }

                return result;
            }

            /**
             * Reset the detector state (but keep calibration)
             */
            function reset() {
                state.holdStartTime = null;
                state.holdDirection = null;
                state.holdProgress = 0;
                state.lastDetectedMovement = null;
                state.movementHistory = [];
                console.log('HeadMovementDetector reset (calibration preserved)');
            }

            /**
             * Full reset including calibration
             */
            function resetCalibration() {
                state = {
                    baseline: {
                        yaw: 0,
                        pitch: 0,
                        isCalibrated: false,
                        calibrationSamples: []
                    },
                    currentYaw: 0,
                    currentPitch: 0,
                    rawYaw: 0,
                    rawPitch: 0,
                    holdStartTime: null,
                    holdDirection: null,
                    holdProgress: 0,
                    lastDetectedMovement: null,
                    movementHistory: []
                };
                console.log('HeadMovementDetector full reset (calibration cleared)');
            }

            /**
             * Manually set baseline to current position
             */
            function calibrate() {
                state.baseline.yaw = state.currentYaw;
                state.baseline.pitch = state.currentPitch;
                state.baseline.isCalibrated = true;
                state.baseline.calibrationSamples = [];
                console.log(`HeadMovementDetector manually calibrated - yaw=${state.baseline.yaw.toFixed(1)}¬∞, pitch=${state.baseline.pitch.toFixed(1)}¬∞`);
            }

            /**
             * Check if calibration is complete
             */
            function isCalibrated() {
                return state.baseline.isCalibrated;
            }

            /**
             * Get current yaw and pitch values
             */
            function getCurrentAngles() {
                return {
                    yaw: state.currentYaw,
                    pitch: state.currentPitch,
                    yawFromBaseline: state.currentYaw - state.baseline.yaw,
                    pitchFromBaseline: state.currentPitch - state.baseline.pitch
                };
            }

            /**
             * Get the baseline values
             */
            function getBaseline() {
                return {
                    yaw: state.baseline.yaw,
                    pitch: state.baseline.pitch,
                    isCalibrated: state.baseline.isCalibrated
                };
            }

            /**
             * Get current hold progress (0-1)
             */
            function getHoldProgress() {
                return state.holdProgress;
            }

            /**
             * Get the current hold direction
             */
            function getHoldDirection() {
                return state.holdDirection;
            }

            /**
             * Get movement history
             */
            function getMovementHistory() {
                return [...state.movementHistory];
            }

            /**
             * Update configuration
             */
            function setConfig(newConfig) {
                Object.assign(config, newConfig);
            }

            /**
             * Get current configuration
             */
            function getConfig() {
                return { ...config };
            }

            // Public API
            return {
                processFrame,
                reset,
                resetCalibration,
                calibrate,
                isCalibrated,
                getCurrentAngles,
                getBaseline,
                getHoldProgress,
                getHoldDirection,
                getMovementHistory,
                setConfig,
                getConfig
            };
        })();

        // =====================================================
        // EXPRESSION DETECTOR MODULE
        // Detects facial expressions: smile, mouth open, eyebrow raise
        // Uses MediaPipe blendshapes for detection
        // =====================================================

        const ExpressionDetector = (function() {
            // Configuration with thresholds from PRD
            const config = {
                // Smile detection threshold (blendshape score > 0.4)
                smileThreshold: 0.4,
                // Mouth open detection threshold (blendshape score > 0.3)
                mouthOpenThreshold: 0.3,
                // Eyebrow raise detection threshold (blendshape score > 0.3)
                eyebrowRaiseThreshold: 0.3,
                // Hold duration required in milliseconds
                holdDuration: 500,
                // Smoothing factor for expression values (0-1, higher = more smoothing)
                smoothingFactor: 0.4,
                // Tolerance for position drift during hold (as fraction of threshold)
                holdTolerance: 0.8
            };

            // Blendshape category names used by MediaPipe
            const blendshapeNames = {
                // Smile detection - use mouth smile blendshapes
                smileLeft: 'mouthSmileLeft',
                smileRight: 'mouthSmileRight',
                // Mouth open detection - use jaw open blendshape
                jawOpen: 'jawOpen',
                // Additional mouth open indicators
                mouthOpen: 'mouthOpen',
                // Eyebrow raise detection
                browInnerUp: 'browInnerUp',
                browOuterUpLeft: 'browOuterUpLeft',
                browOuterUpRight: 'browOuterUpRight'
            };

            // State
            let state = {
                // Current smoothed expression values
                currentValues: {
                    smile: 0,
                    mouthOpen: 0,
                    eyebrowRaise: 0
                },
                // Raw values (before smoothing)
                rawValues: {
                    smile: 0,
                    mouthOpen: 0,
                    eyebrowRaise: 0
                },
                // Individual blendshape scores for debugging
                blendshapeScores: {
                    smileLeft: 0,
                    smileRight: 0,
                    jawOpen: 0,
                    browInnerUp: 0,
                    browOuterUpLeft: 0,
                    browOuterUpRight: 0
                },
                // Hold tracking for each expression type
                holdState: {
                    smile: { startTime: null, progress: 0, isHolding: false },
                    mouthOpen: { startTime: null, progress: 0, isHolding: false },
                    eyebrowRaise: { startTime: null, progress: 0, isHolding: false }
                },
                // Detection history
                detectionHistory: []
            };

            /**
             * Extract blendshape scores from face detection results
             * @param {Array} blendshapes - Array of blendshape categories from MediaPipe
             * @returns {Object} Object with blendshape scores
             */
            function extractBlendshapeScores(blendshapes) {
                const scores = {
                    smileLeft: 0,
                    smileRight: 0,
                    jawOpen: 0,
                    browInnerUp: 0,
                    browOuterUpLeft: 0,
                    browOuterUpRight: 0
                };

                if (!blendshapes || !Array.isArray(blendshapes)) {
                    return scores;
                }

                // Iterate through blendshapes and extract relevant scores
                for (const shape of blendshapes) {
                    switch (shape.categoryName) {
                        case blendshapeNames.smileLeft:
                            scores.smileLeft = shape.score;
                            break;
                        case blendshapeNames.smileRight:
                            scores.smileRight = shape.score;
                            break;
                        case blendshapeNames.jawOpen:
                            scores.jawOpen = shape.score;
                            break;
                        case blendshapeNames.browInnerUp:
                            scores.browInnerUp = shape.score;
                            break;
                        case blendshapeNames.browOuterUpLeft:
                            scores.browOuterUpLeft = shape.score;
                            break;
                        case blendshapeNames.browOuterUpRight:
                            scores.browOuterUpRight = shape.score;
                            break;
                    }
                }

                return scores;
            }

            /**
             * Calculate smile intensity from blendshape scores
             * Uses average of left and right mouth smile
             */
            function calculateSmileIntensity(scores) {
                return (scores.smileLeft + scores.smileRight) / 2;
            }

            /**
             * Calculate mouth open intensity from blendshape scores
             * Uses jaw open blendshape
             */
            function calculateMouthOpenIntensity(scores) {
                return scores.jawOpen;
            }

            /**
             * Calculate eyebrow raise intensity from blendshape scores
             * Uses combination of inner and outer brow raises
             */
            function calculateEyebrowRaiseIntensity(scores) {
                // Weight inner brow more as it's more distinctive
                const innerWeight = 0.5;
                const outerWeight = 0.25;

                return (scores.browInnerUp * innerWeight) +
                       (scores.browOuterUpLeft * outerWeight) +
                       (scores.browOuterUpRight * outerWeight);
            }

            /**
             * Apply exponential smoothing to reduce noise
             */
            function smoothValue(currentSmoothed, newRaw, factor) {
                return currentSmoothed * (1 - factor) + newRaw * factor;
            }

            /**
             * Check if an expression is currently active (above threshold)
             */
            function isExpressionActive(expressionType, value) {
                switch (expressionType) {
                    case 'smile':
                        return value >= config.smileThreshold;
                    case 'mouthOpen':
                        return value >= config.mouthOpenThreshold;
                    case 'eyebrowRaise':
                        return value >= config.eyebrowRaiseThreshold;
                    default:
                        return false;
                }
            }

            /**
             * Get the threshold for an expression type
             */
            function getThreshold(expressionType) {
                switch (expressionType) {
                    case 'smile':
                        return config.smileThreshold;
                    case 'mouthOpen':
                        return config.mouthOpenThreshold;
                    case 'eyebrowRaise':
                        return config.eyebrowRaiseThreshold;
                    default:
                        return 0.5;
                }
            }

            /**
             * Update hold state for a specific expression
             * @returns {Object} Updated hold state with detection result
             */
            function updateHoldState(expressionType, value, now) {
                const holdInfo = state.holdState[expressionType];
                const threshold = getThreshold(expressionType);
                const isActive = value >= threshold;
                const holdThreshold = threshold * config.holdTolerance;
                const isHoldingPosition = value >= holdThreshold;

                const result = {
                    isActive: isActive,
                    isHolding: false,
                    holdProgress: 0,
                    detected: false
                };

                if (isActive) {
                    if (holdInfo.isHolding && isHoldingPosition) {
                        // Continue holding
                        const holdTime = now - holdInfo.startTime;
                        holdInfo.progress = Math.min(holdTime / config.holdDuration, 1);
                        result.holdProgress = holdInfo.progress;
                        result.isHolding = true;

                        // Check if hold is complete
                        if (holdTime >= config.holdDuration) {
                            result.detected = true;
                            // Reset hold to allow detecting again
                            holdInfo.startTime = null;
                            holdInfo.progress = 0;
                            holdInfo.isHolding = false;
                        }
                    } else if (!holdInfo.isHolding) {
                        // Start new hold
                        holdInfo.startTime = now;
                        holdInfo.progress = 0;
                        holdInfo.isHolding = true;
                        result.isHolding = true;
                    } else if (!isHoldingPosition) {
                        // Position dropped below hold tolerance - restart
                        holdInfo.startTime = now;
                        holdInfo.progress = 0;
                    }
                } else {
                    // Expression not active - reset hold
                    holdInfo.startTime = null;
                    holdInfo.progress = 0;
                    holdInfo.isHolding = false;
                }

                return result;
            }

            /**
             * Process a frame and detect facial expressions
             * @param {Object} faceResults - Results from FaceLandmarker.detectForVideo()
             * @returns {Object} Detection result with expression status and metrics
             */
            function processFrame(faceResults) {
                const now = performance.now();

                // Default result
                const result = {
                    // Current expression values (0-1)
                    values: {
                        smile: 0,
                        mouthOpen: 0,
                        eyebrowRaise: 0
                    },
                    // Individual blendshape scores
                    blendshapeScores: state.blendshapeScores,
                    // Expression states (active/holding/detected)
                    expressions: {
                        smile: { isActive: false, isHolding: false, holdProgress: 0, detected: false },
                        mouthOpen: { isActive: false, isHolding: false, holdProgress: 0, detected: false },
                        eyebrowRaise: { isActive: false, isHolding: false, holdProgress: 0, detected: false }
                    },
                    // Overall detection flags
                    anyExpressionDetected: false,
                    detectedExpression: null
                };

                // Check if we have valid face data
                if (!faceResults || !faceResults.faceLandmarks || faceResults.faceLandmarks.length === 0) {
                    // Reset hold states when no face detected
                    Object.keys(state.holdState).forEach(key => {
                        state.holdState[key] = { startTime: null, progress: 0, isHolding: false };
                    });
                    return result;
                }

                // Get blendshapes from face results
                const blendshapes = faceResults.faceBlendshapes && faceResults.faceBlendshapes.length > 0
                    ? faceResults.faceBlendshapes[0].categories
                    : null;

                if (!blendshapes) {
                    return result;
                }

                // Extract blendshape scores
                state.blendshapeScores = extractBlendshapeScores(blendshapes);
                result.blendshapeScores = state.blendshapeScores;

                // Calculate raw expression intensities
                state.rawValues.smile = calculateSmileIntensity(state.blendshapeScores);
                state.rawValues.mouthOpen = calculateMouthOpenIntensity(state.blendshapeScores);
                state.rawValues.eyebrowRaise = calculateEyebrowRaiseIntensity(state.blendshapeScores);

                // Apply smoothing
                state.currentValues.smile = smoothValue(
                    state.currentValues.smile,
                    state.rawValues.smile,
                    config.smoothingFactor
                );
                state.currentValues.mouthOpen = smoothValue(
                    state.currentValues.mouthOpen,
                    state.rawValues.mouthOpen,
                    config.smoothingFactor
                );
                state.currentValues.eyebrowRaise = smoothValue(
                    state.currentValues.eyebrowRaise,
                    state.rawValues.eyebrowRaise,
                    config.smoothingFactor
                );

                // Update result values
                result.values = { ...state.currentValues };

                // Update hold states and check for detections
                const expressionTypes = ['smile', 'mouthOpen', 'eyebrowRaise'];

                for (const expressionType of expressionTypes) {
                    const value = state.currentValues[expressionType];
                    const holdResult = updateHoldState(expressionType, value, now);

                    result.expressions[expressionType] = holdResult;

                    // Track if any expression was detected
                    if (holdResult.detected) {
                        result.anyExpressionDetected = true;
                        result.detectedExpression = expressionType;

                        // Add to detection history
                        state.detectionHistory.push({
                            expression: expressionType,
                            value: value,
                            timestamp: now
                        });

                        console.log(`Expression detected: ${expressionType} (value: ${value.toFixed(2)}, held for ${config.holdDuration}ms)`);
                    }
                }

                return result;
            }

            /**
             * Reset the detector state
             * Call this when starting a new challenge
             */
            function reset() {
                state.currentValues = { smile: 0, mouthOpen: 0, eyebrowRaise: 0 };
                state.rawValues = { smile: 0, mouthOpen: 0, eyebrowRaise: 0 };
                state.holdState = {
                    smile: { startTime: null, progress: 0, isHolding: false },
                    mouthOpen: { startTime: null, progress: 0, isHolding: false },
                    eyebrowRaise: { startTime: null, progress: 0, isHolding: false }
                };
                state.detectionHistory = [];
                console.log('ExpressionDetector reset');
            }

            /**
             * Get current expression values
             */
            function getCurrentValues() {
                return { ...state.currentValues };
            }

            /**
             * Get current blendshape scores
             */
            function getBlendshapeScores() {
                return { ...state.blendshapeScores };
            }

            /**
             * Get hold progress for a specific expression
             */
            function getHoldProgress(expressionType) {
                if (state.holdState[expressionType]) {
                    return state.holdState[expressionType].progress;
                }
                return 0;
            }

            /**
             * Check if a specific expression is currently active
             */
            function isExpressionCurrentlyActive(expressionType) {
                const value = state.currentValues[expressionType];
                return isExpressionActive(expressionType, value);
            }

            /**
             * Check if currently holding a specific expression
             */
            function isHolding(expressionType) {
                if (state.holdState[expressionType]) {
                    return state.holdState[expressionType].isHolding;
                }
                return false;
            }

            /**
             * Get detection history
             */
            function getDetectionHistory() {
                return [...state.detectionHistory];
            }

            /**
             * Update configuration
             */
            function setConfig(newConfig) {
                Object.assign(config, newConfig);
            }

            /**
             * Get current configuration
             */
            function getConfig() {
                return { ...config };
            }

            // Public API
            return {
                processFrame,
                reset,
                getCurrentValues,
                getBlendshapeScores,
                getHoldProgress,
                isExpressionCurrentlyActive,
                isHolding,
                getDetectionHistory,
                setConfig,
                getConfig
            };
        })();

        // Initialize the app on page load
        document.addEventListener('DOMContentLoaded', async () => {
            console.log('Liveness Detection App - Initializing...');

            // Disable button until both camera and model are ready
            elements.controlBtn.disabled = true;

            // Initialize camera and model in parallel for faster startup
            const [cameraResult, modelResult] = await Promise.all([
                initializeCamera(),
                initializeFaceLandmarker()
            ]);

            // Check if both initialized successfully
            if (cameraResult && modelResult) {
                checkAndEnableStartButton();
                // Start detection loop immediately for face positioning feedback
                startDetectionLoop();
            } else {
                if (!cameraResult) {
                    console.error('Camera initialization failed');
                }
                if (!modelResult) {
                    console.error('Model initialization failed');
                }
            }

            // Control button click handler
            elements.controlBtn.addEventListener('click', () => {
                if (!isAppReady()) {
                    console.error('App not ready');
                    return;
                }

                // Start detection if not running
                if (!isDetectionRunning) {
                    startDetectionLoop();
                }

                // Button will be used to start verification challenges
                // (Challenge state machine will be implemented in Task 10)
                console.log('Verification started');
                updateStatus('Verification in progress...');
            });

            // Retry button click handler
            elements.retryBtn.addEventListener('click', () => {
                // Reset and restart (full implementation in later tasks)
                elements.resultScreen.classList.remove('visible');
                elements.challengePanel.classList.remove('hidden');
                elements.controlBtn.classList.remove('hidden');
                startDetectionLoop();
            });
        });
    </script>
</body>
</html>
